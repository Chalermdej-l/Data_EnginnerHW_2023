{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.1 run query select count(*) from `homeworkw3.taxi_2019`\n",
    "# A 2.43244696\n",
    "\n",
    "# Q.2 After create both table hover over below query in Big query\n",
    "# select count(distinct Affiliated_base_number) from `homeworkw3.taxi_2019` \n",
    "# select count(distinct Affiliated_base_number) from `homeworkw3.taxi_2019_ex`\n",
    "\n",
    "# A 4.table 317.94 external 0\n",
    "\n",
    "# Q.3 Run this query to find count of null\n",
    "# select count( *) from `homeworkw3.taxi_2019`  where PUlocationID is null and DOlocationID is null\n",
    "# A 1.717748\n",
    "\n",
    "# Q.4 \n",
    "# A 2.Partition by pickup_datetime Cluster on affiliated_base_number\n",
    "\n",
    "# Q5 run query to cretae a parttion and cluster\n",
    "# CREATE TABLE homeworkw3.taxi_2019_partition\n",
    "# PARTITION BY pickup_date\n",
    "# CLUSTER BY\n",
    "#   Affiliated_base_number\n",
    "# AS\n",
    "# select dispatching_base_num,\n",
    "# cast(pickup_datetime as date) as pickup_date,\n",
    "# cast(dropOff_datetime as date) as dropOff_date,\n",
    "# PUlocationID,DOlocationID,SR_Flag,Affiliated_base_number from `homeworkw3.taxi_2019` \n",
    "\n",
    "# use below query to select distinct number with where cluase\n",
    "# select distinct Affiliated_base_number from `homeworkw3.taxi_2019_partition` where pickup_date between '2019-03-01' and '2019-03-31'\n",
    "# select distinct Affiliated_base_number from `homeworkw3.taxi_2019` where pickup_datetime between '2019-03-01' and '2019-03-31'\n",
    "\n",
    "# A 2.647.87 MB for non-partitioned table and 23.06 MB for the partitioned table\n",
    "\n",
    "# Q6 check from detail tab of external table gs://dataengineer_2023/fhv_tripdata_2019.csv.gz\n",
    "# A 4.GCP Bucket\n",
    "\n",
    "# Q7\n",
    "# A 1.True\n",
    "\n",
    "# Q8 Run below pipline to load the file and upload to gcs and bq\n",
    "from prefect import task,flow\n",
    "from prefect_gcp.cloud_storage import GcsBucket\n",
    "from prefect_gcp import GcpCredentials\n",
    "from prefect_gcp.bigquery import bigquery_create_table\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd  \n",
    "from config import credential\n",
    "task()\n",
    "def writegcs(file_list):\n",
    "    block = GcsBucket.load(\"gcs-dataengineer\")\n",
    "    to_name = file_list[file_list.rfind('\\\\')+1:]\n",
    "    block.upload_from_path(from_path=file_list,to_path=to_name)\n",
    "\n",
    "    return None\n",
    "\n",
    "task() \n",
    "def loadfile(year,month):\n",
    "    url =f'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_{year}-{month:02}.csv.gz'\n",
    "    print(url)\n",
    "    file = pd.read_csv(url)\n",
    "    return file\n",
    "\n",
    "task() \n",
    "def createtable(file,project_id):\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    table = f'{credential.project_id}.dataset.bigquery_pq'\n",
    "    job = client.load_table_from_dataframe(file, table)\n",
    "\n",
    "\n",
    "@flow(name='bq_external')\n",
    "def create_externaltable(project_id):\n",
    "    gcp_credentials = GcpCredentials(project=project_id)    \n",
    "\n",
    "    external = bigquery.ExternalConfig('PARQUET')\n",
    "    external.autodetect = True\n",
    "    external.source_uris = [ 'gs://dataengineer_2023/fhv_tripdata_2019.parquet']\n",
    "    result = bigquery_create_table(\n",
    "        dataset=\"dataset\",\n",
    "        table=\"external_pq\",\n",
    "        location='asia-southeast1',\n",
    "        gcp_credentials=gcp_credentials,\n",
    "        external_config =external    \n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "flow()\n",
    "def mainflow():\n",
    "    # Define Variable\n",
    "    year = 2019\n",
    "    month = [i for i in range(1,13)]\n",
    "    project_id = credential.project_id\n",
    "    file_all = pd.DataFrame()\n",
    "\n",
    "    # Load in the file from Github\n",
    "    for i in month:\n",
    "         file =loadfile(year,i)\n",
    "         file_all = file_all.append(file)\n",
    "    # Create a combine 2019 data in parquet to local\n",
    "    \n",
    "    path = f'data\\\\fhv_tripdata_{year}.parquet'\n",
    "    file_all.to_parquet(path,index=False)\n",
    "\n",
    "    # Upload file to GCS\n",
    "    _ = writegcs(path)\n",
    "\n",
    "    #Create table in BQ   \n",
    "    _ = create_externaltable(project_id) \n",
    "    _ = createtable(file_all,project_id)    \n",
    "    \n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    # Call the pipeline\n",
    "    mainflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "601b049b69c015f341cf049bdfa863242bcd463fff4e9ea5ae96948b9f5d87db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
